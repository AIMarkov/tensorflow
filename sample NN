import numpy as np
import tensorflow as tf
from matplotlib import pyplot as pl
X=tf.placeholder(tf.float32,[None,1])
Y=tf.placeholder(tf.float32,[None,1])

w1=tf.Variable(tf.random_normal([1,3]))
bias1=tf.Variable(tf.random_normal([1,3],stddev=1,seed=1))
w2=tf.Variable(tf.random_normal([3,3]))
bias2=tf.Variable(tf.random_normal([1,3],stddev=1,seed=1))
w3=tf.Variable(tf.random_normal([3,1]))
output1=tf.nn.tanh(tf.matmul(X,w1)+bias1)
output2=tf.nn.tanh(tf.matmul(output1,w2)+bias2)
output=tf.matmul(output2,w3)
cross_entropy=tf.reduce_mean(tf.reduce_sum(tf.square(Y- output),
                     reduction_indices=[1]))
train_step=tf.train.AdamOptimizer(0.01).minimize(cross_entropy)
#train_step=tf.train.GradientDescentOptimizer(0.1).minimize(cross_entropy)

def data(N):
    x=np.random.uniform(0,20,[N,1])
    noise = np.random.normal(0, 0.05, x.shape)
    y=x*2+1+noise
    return  x,y
init=tf.global_variables_initializer()
sess=tf.Session()
sess.run(init)
for i in range(10000):
    x,y=data(10)
    sess.run(train_step,feed_dict={X:x,Y:y})
    loss=sess.run(cross_entropy,feed_dict={X:x,Y:y})
    print("lossfunction:",loss)
x,y=data(100)
pl.plot(x,y,'r.')
predict=sess.run(output,feed_dict={X:x})
pl.plot(x,predict,'y.')
pl.show()
